---
title: '<font color = "black">Private College Acceptance</font>'
subtitle: '<font color = "black">Multiple linear Regression</font>'  
author: 
  - '<font color = "black">Samantha Gouveia and Tyree Brown</font>'
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: true
     
---

```{r setup, include=FALSE}
s <- read.csv("https://raw.githubusercontent.com/sgouveia23/STA321/main/projecct2/College_Data.csv", header = TRUE)

library(psych)
library(mlbench)
library(MASS)
library(kableExtra)
library(pROC)

knitr::opts_chunk$set(echo = FALSE)


    # make a copy of the data for data cleansing
 m <- s[,-c(1,2,6,8,9,11,12,13,14,15,16,17,18)]
```

<h1 align="center"> Data Introduction.</h1>

The case study focuses on 777 private and public colleges with 7 predictors

Response variable:
>Private

Predictors: 
>Number of application

>Acceptance rate

>Total Enrollment 

>Top 25 percent of high school class

>Graduation rate 

>Number of out of state students


---

<h1 align="center">Objective</h1>

<br>
- <font size = 6> Find which predictor variables are the most influential in predicting acceptence to a private college.</font><br>
<br>
<br>

- <font size = 6> Explore the correltation between variables and determine which model will perform the best.</font>

---

<h1 align="center">Pairwise Scatterplot</h1>
```{r pairwise scatterplot, echo = FALSE, fig.width=14, fig.height=7}
pairs.panels(m,method="pearson",hist.col="plum",density=TRUE,ellipses=TRUE )
```
---
<h1 align="center">Comparison of Variables</h1>
.left-column[
<font size = 5>Create two histograms due to their high correlation<br>

to determine which will be used in the predictor models.</font>

>Acceptance

>Application

]


.right-column[
``` {r,echo = FALSE, fig.width=8, fig.height=6}
par(mfrow=c(1,2))

# Plot histogram for "Accept"
hist(m$Accept, xlab = "Accept", main = "Histogram of Accept", col = "skyblue", border = "white")

# Plot histogram for "Apps"
hist(m$Apps, xlab = "Apps", main = "Histogram of Apps", col = "lightcoral", border = "white")
```
]


```{r echo = FALSE, message=FALSE}


cc <- s[,-c(1,6,8,9,11,12,13,14,15,16,17,18)]
cc$Private <- as.factor(cc$Private)
## standardizing numerical variables


cc$sd.Enroll = (cc$Enroll-mean(cc$Enroll))/sd(cc$Enroll)

cc$sd.Top25perc = (cc$Top25perc-mean(cc$Top25perc))/sd(cc$Top25perc)

cc$sd.Grad.Rate = (cc$Grad.Rate-mean(cc$Grad.Rate))/sd(cc$Grad.Rate)

cc$sd.Outstate = (cc$Outstate-mean(cc$Outstate))/sd(cc$Outstate)

cc$sd.Accept = (cc$Accept-mean(cc$Accept))/sd(cc$Accept)

cc$sd.Apps = (cc$Apps-mean(cc$Apps))/sd(cc$Apps)

sd.cc = cc[, -c(2:7)]

```

```{r echo = FALSE, message=FALSE}

n <- dim(sd.cc)[1]
train.n <- round(0.8*n)
train.id <- sample(1:n, train.n, replace = FALSE)

train <- sd.cc[train.id, ]
test <- sd.cc[-train.id, ]

```

---
<h1 align="center">Cross Validation</h1><br><br>

- Standardize the variables<br><br>

- Split data 
 - 80% training data 
 - 20% assesses the performance of the final model<br><br>
 
- 5-fold cross validation


---
<h1 align="center">Prediction Error</h1>

```{r echo = FALSE}

## 5-fold cross-validation
k=5

## floor() function must be used to avoid producing NA in the subsequent results
fold.size = floor(dim(train)[1]/k)

PE1 = rep(0,5)
PE2 = rep(0,5)
PE3 = rep(0,5)


for(i in 1:k){
  
  valid.id = (fold.size*(i-1)+1):(fold.size*i)
  valid = train[valid.id, ]
  train.dat = train[-valid.id,]
  
  
  ##  full model
  candidate01 = glm(Private ~  sd.Top25perc +  sd.Grad.Rate + 
                    sd.Enroll + sd.Outstate + sd.Accept, family = binomial(link = "logit"),  
                    data = train.dat) 
  
  
    candidate03 = glm(Private ~  sd.Enroll + sd.Top25perc + sd.Outstate + sd.Accept , 
                    family = binomial(link = "logit"),  
                    data = train.dat) 
## 
   candidate02 = stepAIC(candidate01, 
                      scope = list(lower=formula(candidate03),upper=formula(candidate01)),
                      direction = "forward",  
                      trace = 0 )               
                      
   
      pred01 = predict(candidate01, newdata = valid, type="response")
   pred02 = predict(candidate02, newdata = valid, type="response")
   pred03 = predict(candidate03, newdata = valid, type="response")
   
   pre.outcome01 = ifelse(as.vector(pred01) > 0.5, "Yes", "No")
   pre.outcome02 = ifelse(as.vector(pred02) > 0.5, "Yes", "No")
   pre.outcome03 = ifelse(as.vector(pred03) > 0.5, "Yes", "No")
   
     PE1[i] = sum(pre.outcome01 == valid$Private )/length(pred01)
   PE2[i] = sum(pre.outcome02 == valid$Private )/length(pred02)
   PE3[i] = sum(pre.outcome03 == valid$Private )/length(pred03)
   
}


avg.pe = cbind(PE1 = mean(PE1), PE2 = mean(PE2), PE3 = mean(PE3))

kable(avg.pe, caption = "Average of prediction errors of candidate models")
```

Create 3 candidate models to find the prediction errors<br>

> Full model

> Reduced model

> Step-wise model

---
<h1 align="center">Accuracy</h1>
<br>
<br>
- <font size = 5>We see the actual accuracy of the final model to be 0.741<br><br>

- The step-wise model will be used as the final model due to having the lowest prediction error and number of variables
</font>


---

<h1 align="center">ROC Curve</h1>
.right-column[
```{r echo = FALSE}

TPR.FPR=function(pred){
  prob.seq = seq(0,1, length=50)  
  pn=length(prob.seq)
  true.lab=as.vector(train$Private)
  TPR = NULL
  FPR = NULL
  ##
  for (i in 1:pn){
   pred.lab = as.vector(ifelse(pred >prob.seq[i],"Yes", "No"))
   TPR[i] = length(which(true.lab=="Yes" & pred.lab=="Yes"))/length(which(true.lab=="Yes"))
   FPR[i] = length(which(true.lab=="No" & pred.lab=="Yes"))/length(which(true.lab=="No"))
  }
   cbind(FPR = FPR, TPR = TPR)
}
```

```{r echo = FALSE, message=FALSE}
##  full model
  candidate01 = glm(Private ~  sd.Enroll + + sd.Top25perc + sd.Grad.Rate + sd.Outstate + sd.Accept, family = binomial(link = "logit"),  
                    data = train) 
## reduced model
    candidate03 = glm(Private ~  sd.Enroll + sd.Top25perc + sd.Outstate + sd.Accept, 
                    family = binomial(link = "logit"),  
                    data = train) 
  
     candidate02 = stepAIC(candidate01, 
                      scope = list(lower=formula(candidate03),upper=formula(candidate01)),
                      direction = "forward",   
                      trace = 0)                
                      
pred01 = predict.glm(candidate01, newdata = train, type="response") 
pred02 = predict.glm(candidate02, newdata = train, type="response")
pred03 = predict.glm(candidate03, newdata = train, type="response")
```

```{r echo=FALSE, message=FALSE}

## ROC curve
 plot(TPR.FPR(pred01)[,1], TPR.FPR(pred01)[,2], 
      type="l", col=2, lty=1, xlim=c(0,1), ylim=c(0,1),
      xlab = "FPR: 1 - specificity",
      ylab ="TPR: sensitivity",
      main = "ROC curves of the three candidate models",
      cex.main = 0.8,
      col.main = "red")
 lines(TPR.FPR(pred02)[,1], TPR.FPR(pred02)[,2],  col=3, lty=2)
 lines(TPR.FPR(pred03)[,1], TPR.FPR(pred03)[,2],  col=4, lty=3)  
 
  ##
  category = train$Private == "Yes"
  ROCobj01 <- roc(category, as.vector(pred01))
  ROCobj02 <- roc(category, as.vector(pred02))
  ROCobj03 <- roc(category, as.vector(pred03))
  AUC01 = round(auc(ROCobj01),4)
  AUC02 = round(auc(ROCobj02),4)
  AUC03 = round(auc(ROCobj03),4)
  ##
  legend("bottomright", c(paste("Full model: AUC = ",AUC01), 
                         paste("Stepwise model: AUC =",AUC02),
                         paste("reduced model: AUC =", AUC03)),
        col=2:4, lty=1:3, cex = 0.8, bty="n")
```
]
.left-column[

<font size = 5>The ROC Curve shows the aggregate measure of performance across all 3 classification thresholds</font>
]

---
<h1 align="center">ROC Curve Analysis</h1>
<br><br><br>
-<font size = 6,h1 align="center"> Find the Full model and Step-wise model have the same AUC at 98% </font></h1><br>
<br>
<br>

-<font size = 6,h1 align="center"> We  see the stepwise model will be the best predictor model</font></h1>

---
<h1 align="center">Conclusion</h1>
<br><br>

- <font size = 6,h1 align="center">The stepwise model is our best predictor model and will be used as the final model
 - Final model = Accept + Enroll + Top 25 percent + Out of state
</font></h1><br>
<br>
<br>

- <font size = 6,h1 align="center">The model has a high prediction rate and a high accuracy rate of acceptance into a private vs public college</font></h1><br>
<br>

---

<h1 align="center">Refernces</h1>
Gupta, Y. (2019, October 28). US College Data. Kaggle. https://www.kaggle.com/datasets/yashgpt/us-college-data



