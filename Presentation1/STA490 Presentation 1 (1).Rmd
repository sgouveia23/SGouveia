---
title: "Private College Acceptance"
subtitle: "Multiple linear Regression"  
author: 
  - "Samantha Gouveia"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: true
---

```{r setup, include=FALSE}
s <- read.csv("https://raw.githubusercontent.com/sgouveia23/STA321/main/projecct2/College_Data.csv", header = TRUE)

library(psych)
library(mlbench)
library(MASS)
library(kableExtra)
library(pROC)

knitr::opts_chunk$set(echo = FALSE)


    # make a copy of the data for data cleansing
 cc <- s[,-c(1,6,8,9,11,12,13,14,15,16,17,18)]

cc$Private <- as.factor(cc$Private)
c <- glm(Private ~ Accept , family = binomial(link = "logit"), data = cc) 

ylimit = max(density(cc$Accept)$y)
hist(cc$Accept, probability = TRUE, main = "Acceptance Distribution", xlab="", 
       col = "azure1", border="lightseagreen")
  lines(density(cc$Accept, adjust=2), col="blue") 
  

s.logit = glm(Private ~ Accept, family = binomial(link ="logit"), data = cc)
 cc$Private <- as.factor(cc$Private)

```

<h1 align="center">Data Introduction</h1>

The case study focuses on 777 private and public colleges with 8 predictors

Response variable:
>Private

Predictors: 
>Application

>Acceptance

>Enrollment

>Top 10 percent

>Top 25 percent

>Grad rate 

---

<h1 align="center">Objective</h1>

> Find which predictor variables are the most influential in predicting acceptence to a private college

>Explore the correltation between variables and determine which model will preform the best.

---

<h1 align="center">Pairwise Scatterplot</h1>
```{r pairwise scatterplot, echo = FALSE, fig.width=14, fig.height=7}
pairs.panels(cc,method="pearson",hist.col="plum",density=TRUE,ellipses=TRUE )
```
---
<h1 align="center">Dummy Variables</h1>
.left-column[
Look at Accept and Apps due to their high correlation
to determine which will be used in the prediction models
]


.right-column[
``` {r,echo = FALSE, fig.width=10, fig.height=6}

par(mfrow=c(1,2))
hist(cc$Accept, xlab = "Accept", main = "")
hist(cc$Apps, xlab = "Apps", main = "")
```
]


```{r echo = FALSE}

## standardizing numerical variables


cc$sd.Enroll = (cc$Enroll-mean(cc$Enroll))/sd(cc$Enroll)

cc$sd.Top25perc = (cc$Top25perc-mean(cc$Top25perc))/sd(cc$Top25perc)

cc$sd.Grad.Rate = (cc$Grad.Rate-mean(cc$Grad.Rate))/sd(cc$Grad.Rate)

cc$sd.Outstate = (cc$Outstate-mean(cc$Outstate))/sd(cc$Outstate)

cc$sd.Accept = (cc$Accept-mean(cc$Accept))/sd(cc$Accept)

cc$sd.Apps = (cc$Apps-mean(cc$Apps))/sd(cc$Apps)

sd.cc = cc[, -c(2:7)]

```

```{r echo = FALSE}

n <- dim(sd.cc)[1]
train.n <- round(0.8*n)
train.id <- sample(1:n, train.n, replace = FALSE)

train <- sd.cc[train.id, ]
test <- sd.cc[-train.id, ]

```
---
<h1 align="center">Prediction Error</h1>


```{r echo = FALSE}
## 5-fold cross-validation
k=5

## floor() function must be used to avoid producing NA in the subsequent results
fold.size = floor(dim(train)[1]/k)

PE1 = rep(0,5)
PE2 = rep(0,5)
PE3 = rep(0,5)


for(i in 1:k){
  
  valid.id = (fold.size*(i-1)+1):(fold.size*i)
  valid = train[valid.id, ]
  train.dat = train[-valid.id,]
  
  
  ##  full model
  candidate01 = glm(Private ~  sd.Top25perc +  sd.Grad.Rate + 
                    sd.Enroll + sd.Outstate + sd.Apps, family = binomial(link = "logit"),  
                    data = train.dat) 
  
  
    candidate03 = glm(Private ~  sd.Enroll + sd.Top25perc + sd.Outstate + sd.Apps , 
                    family = binomial(link = "logit"),  
                    data = train.dat) 
## 
   candidate02 = stepAIC(candidate01, 
                      scope = list(lower=formula(candidate03),upper=formula(candidate01)),
                      direction = "forward",  
                      trace = 0 )               
                      
   
      pred01 = predict(candidate01, newdata = valid, type="response")
   pred02 = predict(candidate02, newdata = valid, type="response")
   pred03 = predict(candidate03, newdata = valid, type="response")
   
   pre.outcome01 = ifelse(as.vector(pred01) > 0.5, "Yes", "No")
   pre.outcome02 = ifelse(as.vector(pred02) > 0.5, "Yes", "No")
   pre.outcome03 = ifelse(as.vector(pred03) > 0.5, "Yes", "No")
   
     PE1[i] = sum(pre.outcome01 == valid$Private )/length(pred01)
   PE2[i] = sum(pre.outcome02 == valid$Private )/length(pred02)
   PE3[i] = sum(pre.outcome03 == valid$Private )/length(pred03)
   
}


avg.pe = cbind(PE1 = mean(PE1), PE2 = mean(PE2), PE3 = mean(PE3))

kable(avg.pe, caption = "Average of prediction errors of candidate models")
```

Choose model 2 for prediction model because it is smaller than model 1
---
<h1 align="center">Accuracy</h1>

```{r echo = FALSE}

pred02 = predict(candidate02, newdata = test, type="response")
pred02.outcome = ifelse(as.vector(pred02)>0.5, "Yes", "No")

accuracy = sum(pre.outcome02 == valid$Private)/length(pred02)
kable(accuracy, caption="The actual accuracy of the final model")
```
We see the actual accuracy the final model will have
---
<h1 align="center">ROC Curve</h1>
```{r echo = FALSE}

TPR.FPR=function(pred){
  prob.seq = seq(0,1, length=50)  
  pn=length(prob.seq)
  true.lab=as.vector(train$Private)
  TPR = NULL
  FPR = NULL
  ##
  for (i in 1:pn){
   pred.lab = as.vector(ifelse(pred >prob.seq[i],"Yes", "No"))
   TPR[i] = length(which(true.lab=="Yes" & pred.lab=="Yes"))/length(which(true.lab=="Yes"))
   FPR[i] = length(which(true.lab=="No" & pred.lab=="Yes"))/length(which(true.lab=="No"))
  }
   cbind(FPR = FPR, TPR = TPR)
}
```

```{r echo = FALSE}
##  full model
  candidate01 = glm(Private ~  sd.Enroll + + sd.Top25perc + sd.Grad.Rate + sd.Outstate + sd.Apps, family = binomial(link = "logit"),  
                    data = train) 
## reduced model
    candidate03 = glm(Private ~  sd.Enroll + sd.Top25perc + sd.Outstate + sd.Apps, 
                    family = binomial(link = "logit"),  
                    data = train) 
  
     candidate02 = stepAIC(candidate01, 
                      scope = list(lower=formula(candidate03),upper=formula(candidate01)),
                      direction = "forward",   
                      trace = 0)                
                      
pred01 = predict.glm(candidate01, newdata = train, type="response") 
pred02 = predict.glm(candidate02, newdata = train, type="response")
pred03 = predict.glm(candidate03, newdata = train, type="response")
```

```{r echo=FALSE}

## ROC curve
 plot(TPR.FPR(pred01)[,1], TPR.FPR(pred01)[,2], 
      type="l", col=2, lty=1, xlim=c(0,1), ylim=c(0,1),
      xlab = "FPR: 1 - specificity",
      ylab ="TPR: sensitivity",
      main = "ROC curves of the three candidate models",
      cex.main = 0.8,
      col.main = "red")
 lines(TPR.FPR(pred02)[,1], TPR.FPR(pred02)[,2],  col=3, lty=2)
 lines(TPR.FPR(pred03)[,1], TPR.FPR(pred03)[,2],  col=4, lty=3)  
 
  ##
  category = train$Private == "Yes"
  ROCobj01 <- roc(category, as.vector(pred01))
  ROCobj02 <- roc(category, as.vector(pred02))
  ROCobj03 <- roc(category, as.vector(pred03))
  AUC01 = round(auc(ROCobj01),4)
  AUC02 = round(auc(ROCobj02),4)
  AUC03 = round(auc(ROCobj03),4)
  ##
  legend("bottomright", c(paste("Full model: AUC = ",AUC01), 
                         paste("Stepwise model: AUC =",AUC02),
                         paste("reduced model: AUC =", AUC03)),
        col=2:4, lty=1:3, cex = 0.8, bty="n")
```

---
<h1 align="center">ROC Curve Analysis</h1>
> Find the Full model and Step-wise model have the same AUC 

> We  see the reduced model will be the worst predictior model

---
<h1 align="center">Conclusion</h1>

>The stepwise model is our best predictor model

>We have a high prediction rate 

>We have a high accuracy
---
<h1 align="center">Refernces</h1>
Gupta, Y. (2019, October 28). US College Data. Kaggle. https://www.kaggle.com/datasets/yashgpt/us-college-data


